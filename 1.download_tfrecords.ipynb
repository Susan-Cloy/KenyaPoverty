{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15dfcf1a",
   "metadata": {},
   "source": [
    "This notebook exports Landsat satellite image composites from Google Earth Engine saved in gzipped TFRecord format (*.tfrecord.gz).\n",
    "\n",
    "For this project, I download satellite images corresponding to 2 different datasets:\n",
    "\n",
    "DHS: 19,669 clusters from DHS surveys, for which we predict cross-sectional (i.e., static in time) cluster-level asset wealth\n",
    "DHSNL: 260,415 locations sampled near DHS survey locations, for which we train transfer learning models to predict nightlights values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c18b94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28296c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\users\\hp\\desktop\\kenya_poverty\n"
     ]
    }
   ],
   "source": [
    "%cd c:/users/hp/desktop/kenya_poverty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d8cb7",
   "metadata": {},
   "source": [
    "# Downloading TFRecords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b70a40c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import math\n",
    "import time \n",
    "import ee\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections.abc import Mapping\n",
    "from typing import Any, Optional\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b19bdda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>To authorize access needed by Earth Engine, open the following\n",
       "        URL in a web browser and follow the instructions:</p>\n",
       "        <p><a href=https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=7eEox6CpucPtkkih8PBCy6lrVzBb4QGPYFRIZ-hMMfM&code_challenge_method=S256>https://accounts.google.com/o/oauth2/auth?client_id=517222506229-vsmmajv00ul0bs7p89v5m89qs8eb9359.apps.googleusercontent.com&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fearthengine+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdevstorage.full_control&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&response_type=code&code_challenge=7eEox6CpucPtkkih8PBCy6lrVzBb4QGPYFRIZ-hMMfM&code_challenge_method=S256</a></p>\n",
       "        <p>The authorization workflow will generate a code, which you\n",
       "        should paste in the box below</p>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter verification code: 4/1AX4XfWhpzjz_y-lUuSqHhkgEj_0MaWDJfZCHsGvWyTKi3cnq8iogrxftugI\n",
      "\n",
      "Successfully saved authorization token.\n"
     ]
    }
   ],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2be6c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3eca2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export location parameters\n",
    "DHS_EXPORT_FOLDER = 'dhs_tfrecords_raw'\n",
    "DHSNL_EXPORT_FOLDER = 'dhsnl_tfrecords_raw'\n",
    "#LSMS_EXPORT_FOLDER = 'lsms_tfrecords_raw'\n",
    "\n",
    "# input data paths\n",
    "DHS_CSV_PATH = 'data/ke_dhs_clusters.csv'\n",
    "DHSNL_CSV_PATH = 'data/ke_dhsnl_locs.csv'\n",
    "#LSMS_CSV_PATH = 'data/lsms_clusters.csv'\n",
    "\n",
    "# band names\n",
    "MS_BANDS = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "# image parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "SCALE = 30                # export resolution: 30m/px\n",
    "EXPORT_TILE_RADIUS = 127  # image dimension = (2*EXPORT_TILE_RADIUS) + 1 = 255px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7981b393",
   "metadata": {},
   "source": [
    "# Setting up utils for downloading data from Google Earth Engine API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b27e8648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_fc(df, lat_colname='lat', lon_colname='lon'):\n",
    "    '''Create a ee.FeatureCollection from a pd.DataFrame.\n",
    "    Args\n",
    "    - csv_path: str, path to CSV file that includes at least two columns for\n",
    "        latitude and longitude coordinates\n",
    "    - lat_colname: str, name of latitude column\n",
    "    - lon_colname: str, name of longitude column\n",
    "    Returns: ee.FeatureCollection, contains one feature per row in the CSV file\n",
    "    '''\n",
    "    df = df.astype('object')\n",
    "    ee_features = []\n",
    "    for i in range(len(df)):\n",
    "        props = df.iloc[i].to_dict()\n",
    "\n",
    "        _geometry = ee.Geometry.Point([props[lon_colname], props[lat_colname],])\n",
    "        ee_feat = ee.Feature(_geometry, props)\n",
    "        ee_features.append(ee_feat)\n",
    "        \n",
    "    return ee.FeatureCollection(ee_features)\n",
    "\n",
    "\n",
    "def surveyyear_to_range(survey_year, nl=False):\n",
    "    '''Returns the start and end dates for filtering satellite images for a survey beginning in the specified year.\n",
    "    Args\n",
    "    - survey_year: int, year that survey was started\n",
    "    - nl: bool, whether to use special range for night lights\n",
    "    Returns\n",
    "    - start_date: str, start date for filtering satellite images (yyyy-mm-dd)\n",
    "    - end_date: str, end date for filtering satellite images (yyyy-mm-dd)\n",
    "    '''\n",
    "    if 2009 <= survey_year and survey_year <= 2011:\n",
    "        start_date = '2009-1-1'\n",
    "        end_date = '2011-12-31'\n",
    "    elif 2012 <= survey_year and survey_year <= 2014:\n",
    "        start_date = '2012-1-1'\n",
    "        end_date = '2014-12-31'\n",
    "    elif 2015 <= survey_year and survey_year <= 2017:\n",
    "        start_date = '2015-1-1'\n",
    "        end_date = '2017-12-31'\n",
    "    else:\n",
    "        raise ValueError(f'Invalid survey_year: {survey_year}.\\nMust be between 2009 and 2017 (inclusive)')\n",
    "    return start_date, end_date\n",
    "\n",
    "def decode_qamask(img: ee.Image) -> ee.Image:\n",
    "    '''\n",
    "    Args\n",
    "    - img: ee.Image, Landsat 5/7/8 image containing 'pixel_qa' band\n",
    "    Returns\n",
    "    - masks: ee.Image, contains 5 bands of masks\n",
    "    Pixel QA Bit Flags (universal across Landsat 5/7/8)\n",
    "    Bit  Attribute\n",
    "    0    Fill\n",
    "    1    Clear\n",
    "    2    Water\n",
    "    3    Cloud Shadow\n",
    "    4    Snow\n",
    "    5    Cloud\n",
    "    '''\n",
    "    qa = img.select('pixel_qa')\n",
    "    clear = qa.bitwiseAnd(2).neq(0)  # 0 = not clear, 1 = clear\n",
    "    clear = clear.updateMask(clear).rename(['pxqa_clear'])\n",
    "\n",
    "    water = qa.bitwiseAnd(4).neq(0)  # 0 = not water, 1 = water\n",
    "    water = water.updateMask(water).rename(['pxqa_water'])\n",
    "\n",
    "    cloud_shadow = qa.bitwiseAnd(8).eq(0)  # 0 = shadow, 1 = not shadow\n",
    "    cloud_shadow = cloud_shadow.updateMask(cloud_shadow).rename(['pxqa_cloudshadow'])\n",
    "\n",
    "    snow = qa.bitwiseAnd(16).eq(0)  # 0 = snow, 1 = not snow\n",
    "    snow = snow.updateMask(snow).rename(['pxqa_snow'])\n",
    "\n",
    "    cloud = qa.bitwiseAnd(32).eq(0)  # 0 = cloud, 1 = not cloud\n",
    "    cloud = cloud.updateMask(cloud).rename(['pxqa_cloud'])\n",
    "\n",
    "    masks = ee.Image.cat([clear, water, cloud_shadow, snow, cloud])\n",
    "    return masks\n",
    "\n",
    "def mask_qaclear(img: ee.Image) -> ee.Image:\n",
    "    '''\n",
    "    Args\n",
    "    - img: ee.Image, Landsat 5/7/8 image containing 'pixel_qa' band\n",
    "    Returns\n",
    "    - img: ee.Image, input image with cloud-shadow, snow, cloud, and unclear\n",
    "        pixels masked out\n",
    "    '''\n",
    "    qam = decode_qamask(img)\n",
    "    cloudshadow_mask = qam.select('pxqa_cloudshadow')\n",
    "    snow_mask = qam.select('pxqa_snow')\n",
    "    cloud_mask = qam.select('pxqa_cloud')\n",
    "    return img.updateMask(cloudshadow_mask).updateMask(snow_mask).updateMask(cloud_mask)\n",
    "\n",
    "\n",
    "def add_latlon(img: ee.Image) -> ee.Image:\n",
    "    '''Creates a new ee.Image with 2 added bands of longitude and latitude coordinates named 'LON' and 'LAT', respectively\n",
    "    '''\n",
    "    latlon = ee.Image.pixelLonLat().select(opt_selectors=['longitude', 'latitude'], opt_names=['LON', 'LAT'])\n",
    "    return img.addBands(latlon)\n",
    "\n",
    "\n",
    "def composite_nl(year: int) -> ee.Image:\n",
    "    '''Creates a median-composite nightlights (NL) image.\n",
    "    Args\n",
    "    - year: int, start year of survey\n",
    "    Returns: ee.Image, contains a single band named 'NIGHTLIGHTS'\n",
    "    '''\n",
    "    img_col = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMSLCFG')\n",
    "\n",
    "    start_date, end_date = surveyyear_to_range(year, nl=True)\n",
    "    return img_col.filterDate(start_date, end_date).median().select([0], ['NIGHTLIGHTS'])\n",
    "\n",
    "\n",
    "def tfexporter(collection, export, prefix, fname, selectors=None,dropselectors=None, bucket=None):\n",
    "    '''Creates and starts a task to export a ee.FeatureCollection to a TFRecord file in Google Drive or Google Cloud \n",
    "        Storage.\n",
    "    Args\n",
    "    - collection: ee.FeatureCollection\n",
    "    - export: str, 'drive' for Drive, 'gcs' for GCS\n",
    "    - prefix: str, folder name in Drive or GCS to export to, no trailing '/'\n",
    "    - fname: str, filename\n",
    "    - selectors: None or ee.List of str, names of properties to include in\n",
    "        output, set to None to include all properties\n",
    "    - dropselectors: None or ee.List of str, names of properties to exclude\n",
    "    - bucket: None or str, name of GCS bucket, only used if export=='gcs'\n",
    "    Returns\n",
    "    - task: ee.batch.Task\n",
    "    '''\n",
    "    if dropselectors is not None:\n",
    "        if selectors is None:\n",
    "            selectors = collection.first().propertyNames()\n",
    "        selectors = selectors.removeAll(dropselectors)\n",
    "\n",
    "    if export == 'gcs':\n",
    "        task = ee.batch.Export.table.toCloudStorage(collection=collection, description=fname, bucket=bucket,\n",
    "                                                    fileNamePrefix=f'{prefix}/{fname}', fileFormat='TFRecord',\n",
    "                                                    selectors=selectors)\n",
    "    elif export == 'drive':\n",
    "        task = ee.batch.Export.table.toDrive(collection=collection, description=fname, folder=prefix, fileNamePrefix=fname,\n",
    "                                             fileFormat='TFRecord', selectors=selectors)\n",
    "    else:\n",
    "        raise ValueError(f'export \"{export}\" is not one of [\"gcs\", \"drive\"]')\n",
    "\n",
    "    task.start()\n",
    "    return task\n",
    "\n",
    "\n",
    "def sample_patch(point, patches_array, scale):\n",
    "    '''Extracts an image patch at a specific point.\n",
    "    Args\n",
    "    - point: ee.Feature\n",
    "    - patches_array: ee.Image, Array Image\n",
    "    - scale: int or float, scale in meters of the projection to sample in\n",
    "    Returns: ee.Feature, 1 property per band from the input image\n",
    "    '''\n",
    "    arrays_samples = patches_array.sample(region=point.geometry(), scale=scale, projection='EPSG:3857', factor=None,\n",
    "                                          numPixels=None, dropNulls=False, tileScale=12)\n",
    "    return arrays_samples.first().copyProperties(point)\n",
    "\n",
    "\n",
    "def get_array_patches(img: ee.Image, scale, ksize, points, export, prefix, fname, selectors=None, dropselectors=None,\n",
    "                      bucket=None):\n",
    "    '''Creates and starts a task to export square image patches in TFRecord format to Google Drive or Google Cloud Storage\n",
    "    (GCS). The image patches are sampled from the given ee.Image at specific coordinates.\n",
    "    Args\n",
    "    - img: ee.Image, image covering the entire region of interest\n",
    "    - scale: int or float, scale in meters of the projection to sample in\n",
    "    - ksize: int or float, radius of square image patch\n",
    "    - points: ee.FeatureCollection, coordinates from which to sample patches\n",
    "    - export: str, 'drive' for Google Drive, 'gcs' for GCS\n",
    "    - prefix: str, folder name in Drive or GCS to export to, no trailing '/'\n",
    "    - fname: str, filename for export\n",
    "    - selectors: None or ee.List, names of properties to include in output,\n",
    "        set to None to include all properties\n",
    "    - dropselectors: None or ee.List, names of properties to exclude\n",
    "    - bucket: None or str, name of GCS bucket, only used if export=='gcs'\n",
    "    Returns: ee.batch.Task\n",
    "    '''\n",
    "    kern = ee.Kernel.square(radius=ksize, units='pixels')\n",
    "    patches_array = img.neighborhoodToArray(kern)\n",
    "\n",
    "    # ee.Image.sampleRegions() does not cut it for larger collections,\n",
    "    # using mapped sample instead\n",
    "    samples = points.map(lambda pt: sample_patch(pt, patches_array, scale))\n",
    "\n",
    "    # export to a TFRecord file which can be loaded directly in TensorFlow\n",
    "    return tfexporter(collection=samples, export=export, prefix=prefix, fname=fname, selectors=selectors,\n",
    "                      dropselectors=dropselectors, bucket=bucket)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874b0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandsatSR:\n",
    "    def __init__(self, filterpoly, start_date, end_date):\n",
    "        '''\n",
    "        Args\n",
    "        - filterpoly: ee.Geometry\n",
    "        - start_date: str, string representation of start date\n",
    "        - end_date: str, string representation of end date\n",
    "        '''\n",
    "        self.filterpoly = filterpoly\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.l8 = self.init_coll('LANDSAT/LC08/C01/T1_SR').map(self.rename_l8).map(self.rescale_l8)\n",
    "        self.l7 = self.init_coll('LANDSAT/LE07/C01/T1_SR').map(self.rename_l57).map(self.rescale_l57)\n",
    "        self.l5 = self.init_coll('LANDSAT/LT05/C01/T1_SR').map(self.rename_l57).map(self.rescale_l57)\n",
    "        self.merged = self.l5.merge(self.l7).merge(self.l8).sort('system:time_start')\n",
    "\n",
    "    def init_coll(self, name: str) -> ee.ImageCollection:\n",
    "        '''\n",
    "        Creates a ee.ImageCollection containing images of desired points\n",
    "        between the desired start and end dates.\n",
    "        Args\n",
    "        - name: str, name of collection\n",
    "        Returns: ee.ImageCollection\n",
    "        '''\n",
    "        return (ee.ImageCollection(name).filterBounds(self.filterpoly).filterDate(self.start_date, self.end_date))\n",
    "\n",
    "    @staticmethod\n",
    "    def rename_l8(img: ee.Image) -> ee.Image:\n",
    "        '''\n",
    "        Args\n",
    "        - img: ee.Image, Landsat 8 image\n",
    "        Returns\n",
    "        - img: ee.Image, with bands renamed\n",
    "        See: https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LC08_C01_T1_SR\n",
    "\n",
    "        Name       Scale Factor Description\n",
    "        B1         0.0001       Band 1 (Ultra Blue) surface reflectance, 0.435-0.451 um\n",
    "        B2         0.0001       Band 2 (Blue) surface reflectance, 0.452-0.512 um\n",
    "        B3         0.0001       Band 3 (Green) surface reflectance, 0.533-0.590 um\n",
    "        B4         0.0001       Band 4 (Red) surface reflectance, 0.636-0.673 um\n",
    "        B5         0.0001       Band 5 (Near Infrared) surface reflectance, 0.851-0.879 um\n",
    "        B6         0.0001       Band 6 (Shortwave Infrared 1) surface reflectance, 1.566-1.651 um\n",
    "        B7         0.0001       Band 7 (Shortwave Infrared 2) surface reflectance, 2.107-2.294 um\n",
    "        B10        0.1          Band 10 brightness temperature (Kelvin), 10.60-11.19 um\n",
    "        B11        0.1          Band 11 brightness temperature (Kelvin), 11.50-12.51 um\n",
    "        sr_aerosol              Aerosol attributes, see Aerosol QA table\n",
    "        pixel_qa                Pixel quality attributes, see Pixel QA table\n",
    "        radsat_qa               Radiometric saturation QA, see Radsat QA table\n",
    "        '''\n",
    "        newnames = ['AEROS', 'BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1', 'TEMP2', 'sr_aerosol', 'pixel_qa', \n",
    "                    'radsat_qa']\n",
    "        return img.rename(newnames)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_l8(img: ee.Image) -> ee.Image:\n",
    "        '''\n",
    "        Args\n",
    "        - img: ee.Image, Landsat 8 image, with bands already renamed\n",
    "            by rename_l8()\n",
    "        Returns\n",
    "        - img: ee.Image, with bands rescaled\n",
    "        '''\n",
    "        opt = img.select(['AEROS', 'BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2'])\n",
    "        therm = img.select(['TEMP1', 'TEMP2'])\n",
    "        masks = img.select(['sr_aerosol', 'pixel_qa', 'radsat_qa'])\n",
    "\n",
    "        opt = opt.multiply(0.0001)\n",
    "        therm = therm.multiply(0.1)\n",
    "\n",
    "        scaled = ee.Image.cat([opt, therm, masks]).copyProperties(img)\n",
    "        # system properties are not copied\n",
    "        scaled = scaled.set('system:time_start', img.get('system:time_start'))\n",
    "        return scaled\n",
    "\n",
    "    @staticmethod\n",
    "    def rename_l57(img: ee.Image) -> ee.Image:\n",
    "        '''\n",
    "        Args\n",
    "        - img: ee.Image, Landsat 5/7 image\n",
    "        Returns\n",
    "        - img: ee.Image, with bands renamed\n",
    "        \n",
    "        See: https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LT05_C01_T1_SR\n",
    "             https://developers.google.com/earth-engine/datasets/catalog/LANDSAT_LE07_C01_T1_SR\n",
    "\n",
    "        Name             Scale Factor Description\n",
    "        B1               0.0001       Band 1 (blue) surface reflectance, 0.45-0.52 um\n",
    "        B2               0.0001       Band 2 (green) surface reflectance, 0.52-0.60 um\n",
    "        B3               0.0001       Band 3 (red) surface reflectance, 0.63-0.69 um\n",
    "        B4               0.0001       Band 4 (near infrared) surface reflectance, 0.77-0.90 um\n",
    "        B5               0.0001       Band 5 (shortwave infrared 1) surface reflectance, 1.55-1.75 um\n",
    "        B6               0.1          Band 6 brightness temperature (Kelvin), 10.40-12.50 um\n",
    "        B7               0.0001       Band 7 (shortwave infrared 2) surface reflectance, 2.08-2.35 um\n",
    "        sr_atmos_opacity 0.001        Atmospheric opacity; < 0.1 = clear; 0.1 - 0.3 = average; > 0.3 = hazy\n",
    "        sr_cloud_qa                   Cloud quality attributes, see SR Cloud QA table. Note:\n",
    "                                          pixel_qa is likely to present more accurate results\n",
    "                                          than sr_cloud_qa for cloud masking. See page 14 in\n",
    "                                          the LEDAPS product guide.\n",
    "        pixel_qa                      Pixel quality attributes generated from the CFMASK algorithm,\n",
    "                                          see Pixel QA table\n",
    "        radsat_qa                     Radiometric saturation QA, see Radiometric Saturation QA table\n",
    "        '''\n",
    "        newnames = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'TEMP1', 'SWIR2', 'sr_atmos_opacity', 'sr_cloud_qa', \n",
    "                    'pixel_qa', 'radsat_qa']\n",
    "        return img.rename(newnames)\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_l57(img: ee.Image) -> ee.Image:\n",
    "        '''\n",
    "        Args\n",
    "        - img: ee.Image, Landsat 5/7 image, with bands already renamed\n",
    "            by rename_157()\n",
    "        Returns\n",
    "        - img: ee.Image, with bands rescaled\n",
    "        '''\n",
    "        opt = img.select(['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2'])\n",
    "        atmos = img.select(['sr_atmos_opacity'])\n",
    "        therm = img.select(['TEMP1'])\n",
    "        masks = img.select(['sr_cloud_qa', 'pixel_qa', 'radsat_qa'])\n",
    "\n",
    "        opt = opt.multiply(0.0001)\n",
    "        atmos = atmos.multiply(0.001)\n",
    "        therm = therm.multiply(0.1)\n",
    "\n",
    "        scaled = ee.Image.cat([opt, therm, masks, atmos]).copyProperties(img)\n",
    "        # system properties are not copied\n",
    "        scaled = scaled.set('system:time_start', img.get('system:time_start'))\n",
    "        return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "893e9348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(df, country, year, export_folder):\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year) to ee.batch.Task\n",
    "    '''\n",
    "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    \n",
    "    tasks = {}\n",
    "\n",
    "    fc = df_to_fc(subset_df)\n",
    "    start_date, end_date = surveyyear_to_range(year)\n",
    "\n",
    "    # create 3-year Landsat composite image\n",
    "    roi = fc.geometry()\n",
    "    imgcol = LandsatSR(roi, start_date=start_date, end_date=end_date).merged\n",
    "    imgcol = imgcol.map(mask_qaclear).select(MS_BANDS)\n",
    "    img = imgcol.median()\n",
    "\n",
    "    # add nightlights, latitude, and longitude bands\n",
    "    img = add_latlon(img)\n",
    "    img = img.addBands(composite_nl(year))\n",
    "\n",
    "    fname = f'{country}_{year}'\n",
    "    tasks[(export_folder, country, year)] = get_array_patches(img=img, scale=SCALE, ksize=EXPORT_TILE_RADIUS, points=fc, \n",
    "                                                              export='drive', fname=fname, bucket=None, prefix=export_folder)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac056553",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27960aeb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False)\\ndhs_surveys = list(dhs_df.groupby(['country', 'year']).groups.keys())\\n\\nfor country, year in dhs_surveys:\\n    new_tasks = export_images(df=dhs_df, country=country, year=year, export_folder=DHS_EXPORT_FOLDER)\\n    tasks.update(new_tasks)\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False)\n",
    "dhs_surveys = list(dhs_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in dhs_surveys:\n",
    "    new_tasks = export_images(df=dhs_df, country=country, year=year, export_folder=DHS_EXPORT_FOLDER)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7352ce64",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dhsnl_df = pd.read_csv(DHSNL_CSV_PATH, float_precision='high', index_col=False)\\ndhsnl_surveys = list(dhsnl_df.groupby(['country', 'year']).groups.keys())\\n\\nfor country, year in dhsnl_surveys:\\n    new_tasks = export_images(df=dhsnl_df, country=country, year=year, export_folder=DHSNL_EXPORT_FOLDER)\\n    tasks.update(new_tasks)\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dhsnl_df = pd.read_csv(DHSNL_CSV_PATH, float_precision='high', index_col=False)\n",
    "dhsnl_surveys = list(dhsnl_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in dhsnl_surveys:\n",
    "    new_tasks = export_images(df=dhsnl_df, country=country, year=year, export_folder=DHSNL_EXPORT_FOLDER)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1668fa43",
   "metadata": {},
   "source": [
    "# Downloading shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5699d804",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "\n",
    "# This Bash script downloads shapefiles from GADM v3.6 into the\n",
    "# data/shapefiles/ directory.\n",
    "#\n",
    "# Run this script from within the preprocessing/ directory.\n",
    "#\n",
    "# Prerequisites: None.\n",
    "\n",
    "\n",
    "africa_country_codes=(\"KEN\") # Kenya\n",
    "\n",
    "\n",
    "dhs_country_codes=(\"KEN\")  # Kenya)\n",
    "\n",
    "echo \"Getting shapefiles for KEN\"\n",
    "\n",
    "# download ZIP'ed shapefiles from GADM v3.6\n",
    "wget --no-verbose --show-progress \"https://biogeo.ucdavis.edu/data/gadm3.6/shp/gadm36_KEN_shp.zip\"\n",
    "\n",
    "unzip -o \"gadm36_KEN_shp.zip\" *_2.* -d \"gadm36_KEN_shp\"\n",
    "\n",
    "# delete the zip file\n",
    "rm \"gadm36_${code}_shp.zip\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc56ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.mkdir('data/shapefiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec8caf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\Desktop\\kenya_poverty\\data\\shapefiles\n"
     ]
    }
   ],
   "source": [
    "%cd data/shapefiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e99fd85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\HP\\\\Desktop\\\\kenya_poverty\\\\data\\\\shapefiles'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85fdeeee",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Downloading wget-3.2.zip (10 kB)\n",
      "Building wheels for collected packages: wget\n",
      "  Building wheel for wget (setup.py): started\n",
      "  Building wheel for wget (setup.py): finished with status 'done'\n",
      "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9680 sha256=2f000c0a56a09737cd61747e7756e5cd272c5fbbdd997ced9abf69373bb5de95\n",
      "  Stored in directory: c:\\users\\hp\\appdata\\local\\pip\\cache\\wheels\\bd\\a8\\c3\\3cf2c14a1837a4e04bd98631724e81f33f462d86a1d895fae0\n",
      "Successfully built wget\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a8c34f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [........................................................................] 20994263 / 20994263"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gadm36_KEN_shp.zip'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wget\n",
    "\n",
    "url = \"https://biogeo.ucdavis.edu/data/gadm3.6/shp/gadm36_KEN_shp.zip\"\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cd8f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "with zipfile.ZipFile('gadm36_KEN_shp.zip', 'r') as zipf:\n",
    "    zipf.extractall()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
